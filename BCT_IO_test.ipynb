{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I/O test on large dataset and full 12M BCT test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='.\\\\io_test\\\\fair_cut.csv'\n",
    "shards_path='.\\\\io_test\\\\shards'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "head=pd.read_csv(path, nrows=10)\n",
    "cols=head.columns.to_list()\n",
    "print(len(cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DISSECT DATASET\n",
    "def dissect(path_of_jumbo_ds, mem_limit, where_to_store):\n",
    "    count=0\n",
    "    with pd.read_csv(path_of_jumbo_ds, chunksize=mem_limit) as reader:\n",
    "        for c in reader:\n",
    "            c.to_csv(where_to_store + '\\\\shard_{0}.csv'.format(count), mode='w', header=True, index=False)\n",
    "            count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dissect(path, 100_000, shards_path) #create data shards 100K each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 8, 9, 10, 11, 13, 14, 22, 40, 47, 48, 49, 51, 55, 56, 67, 70] [79]\n"
     ]
    }
   ],
   "source": [
    "row = pd.read_csv('.\\\\io_test\\\\shards\\\\shard_0.csv',\n",
    "                    skiprows=0,\n",
    "                    nrows=1)\n",
    "keep_after_anova=['Src Port',\n",
    "'Dst Port',\n",
    "'Protocol',\n",
    "'Flow Duration',\n",
    "'Fwd Pkt Len Max',\n",
    "'Fwd Pkt Len Min',\n",
    "'Fwd Pkt Len Mean',\n",
    "'Fwd Pkt Len Std',\n",
    "'Bwd Pkt Len Min',\n",
    "'Bwd Pkt Len Mean',\n",
    "'Fwd IAT Tot',\n",
    "'Pkt Len Min',\n",
    "'RST Flag Cnt',\n",
    "'PSH Flag Cnt',\n",
    "'ACK Flag Cnt',\n",
    "'CWE Flag Count',\n",
    "'Fwd Seg Size Avg',\n",
    "'Bwd Seg Size Avg',\n",
    "'Init Fwd Win Byts',\n",
    "'Fwd Seg Size Min']\n",
    "relevant_x_idx= [i for i, c in enumerate(row.columns.tolist()) if c in keep_after_anova]\n",
    "\n",
    "relevant_y_idx = [i for i, c in enumerate(row.columns.tolist()) if c == 'Label']\n",
    "\n",
    "print(relevant_x_idx, relevant_y_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "f:\\miniconda\\envs\\dl\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.0\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "%store -r scaler2\n",
    "scaler = scaler2\n",
    "#scaler.mean_ = scaler_data2[0]\n",
    "#scaler.var_ = scaler_data2[1]\n",
    "#scaler.scale_ = scaler_data2[2]\n",
    "#print(scaler_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDatasetShard(Dataset):\n",
    "    def __init__(self, shard_path, features_cols, target, onehot, scaler, shard_length = None):\n",
    "        self.x_cols, self.y_cols, = features_cols, target\n",
    "        self.path = shard_path\n",
    "        self.len = shard_length if shard_length != None else self.__get_shard_len__()\n",
    "        self.onehot = onehot\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = pd.read_csv(self.path,\n",
    "                          skiprows=index,\n",
    "                          nrows=1)\n",
    "        row_x = row.iloc[:, self.x_cols]\n",
    "        tensor_label = self.onehot[row.iloc[0, self.y_cols][0]]\n",
    "        return torch.tensor(scaler.transform(row_x.to_numpy().reshape(1,-1)).reshape(20), dtype=torch.float32), tensor_label\n",
    "\n",
    "    def __get_shard_len__(self):\n",
    "        return len(pd.read_csv(self.path))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cache():\n",
    "    def __init__(self) -> None:\n",
    "        self.currently_loaded = None\n",
    "        self.dataframe_loaded = None\n",
    "    \n",
    "    def cache_shard(self, path, df):\n",
    "        self.dataframe_loaded = df\n",
    "        self.currently_loaded = path\n",
    "    \n",
    "    def is_cached(self, path):\n",
    "        if self.currently_loaded is None:\n",
    "            return False\n",
    "        \n",
    "        return path == self.currently_loaded\n",
    "\n",
    "cache = Cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CachedCustomDatasetShard(Dataset):\n",
    "    def __init__(self, shard_path, features_cols, target, onehot, scaler, cache: Cache = None, shard_length = None):\n",
    "        self.cache = cache\n",
    "        self.x_cols, self.y_cols, = features_cols, target\n",
    "        self.path = shard_path\n",
    "        self.len = shard_length if shard_length != None else self.__get_shard_len__()\n",
    "        self.onehot = onehot\n",
    "        self.scaler = scaler\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.cache is None:\n",
    "            row = pd.read_csv(self.path,\n",
    "                          skiprows=index,\n",
    "                          nrows=1)\n",
    "            row_x = row.iloc[:, self.x_cols]\n",
    "            tensor_label = self.onehot[row.iloc[0, self.y_cols][0]]\n",
    "            return torch.tensor(scaler.transform(row_x.to_numpy().reshape(1,-1)).reshape(20), dtype=torch.float32), tensor_label\n",
    "        \n",
    "        #we have cache\n",
    "        if not self.cache.is_cached(self.path): #shard is not cached\n",
    "            df = pd.read_csv(self.path)\n",
    "            self.cache.cache_shard(self.path, df) #cache shard\n",
    "\n",
    "        row_x = self.cache.dataframe_loaded.iloc[index, self.x_cols]\n",
    "        tensor_label = self.onehot[self.cache.dataframe_loaded.iloc[index, self.y_cols][0]]\n",
    "        return torch.tensor(scaler.transform(row_x.to_numpy().reshape(1,-1)).reshape(20), dtype=torch.float32), tensor_label \n",
    "\n",
    "    def __get_shard_len__(self):\n",
    "        return len(pd.read_csv(self.path))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "shards = []\n",
    "#sh_len = [1_000_000,1_000_000,1_000_000,1_000_000,1_000_000,1_000_000,1_000_000,1_000_000,1_000_000,1_000_000,1_000_000, 548_461]\n",
    "for dirname, _, filenames in os.walk(shards_path):\n",
    "    for filename in filenames:\n",
    "        shard = os.path.join(dirname, filename)\n",
    "        shards.append(shard)\n",
    "\n",
    "\n",
    "dict_={'ddos' : torch.tensor([1,0], dtype=torch.float32),'Benign': torch.tensor([0,1],dtype=torch.float32)}\n",
    "dss = []\n",
    "#for sh, l in zip(shards, sh_len):\n",
    "for sh in shards:\n",
    "    #dss.append(CustomDatasetShard(sh, list(range(20)), [20], dict_, scaler, l))#list(range(0,79)) [79]\n",
    "    dss.append(CachedCustomDatasetShard(sh, relevant_x_idx, relevant_y_idx, dict_, scaler, cache=cache, shard_length=None))#list(range(0,79)) [79]\n",
    "\n",
    "chain_ds=torch.utils.data.ConcatDataset(dss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=DataLoader(chain_ds, shuffle=False, batch_size=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for features, targets in loader:\n",
    "    print(features.shape, targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttackNet(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AttackNet, self).__init__()\n",
    "        self.model=torch.nn.Sequential(\n",
    "            torch.nn.Linear(20, 512),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(512, 256),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(256, 128),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.Tanh(),\n",
    "            torch.nn.Linear(64, 2),\n",
    "            torch.nn.LogSoftmax(dim=1)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=AttackNet()\n",
    "model.load_state_dict(torch.load('.\\\\models\\\\02-10-2023_11-38-49__anova_binary_opt_sched_on_fair.model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda:0'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print flush is not working well on jupyter notebook: last output that is per class accuracy is the first showed..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running accuracy: [0.997919107205187, 0.9962137186555163] . Seen 12548460 samples..[0.9960423157514734, 0.9962236249982255] . Seen 6775000 samples. [0.9960423157514734, 0.9962232928460997] . Seen 6778600 samples. [0.9960423157514734, 0.9962268972167785] . Seen 7049000 samples. [0.9960423157514734, 0.9962197530544067] . Seen 7176200 samples. [0.9960423157514734, 0.9962178749024225] . Seen 7185800 samples. [0.9960423157514734, 0.9962138350526312] . Seen 7490400 samples. [0.9960423157514734, 0.9962129248950148] . Seen 7544400 samples. [0.9960423157514734, 0.9962173398707479] . Seen 7806400 samples. [0.9960423157514734, 0.9962178155396962] . Seen 7814200 samples. [0.9960423157514734, 0.9962176461905862] . Seen 7833000 samples. [0.9960423157514734, 0.9962182496712092] . Seen 7886200 samples. [0.9960423157514734, 0.9962190526967406] . Seen 8137800 samples. [0.9960423157514734, 0.9962190024480895] . Seen 8161000 samples. [0.9960423157514734, 0.9962239658552996] . Seen 8434000 samples. [0.9960423157514734, 0.9962206491305505] . Seen 8484400 samples. [0.9956434844856593, 0.9962190943435388] . Seen 8726200 samples. [0.9957855995388027, 0.9962190943435388] . Seen 8819000 samples. [0.9960174330730109, 0.9962190943435388] . Seen 8984600 samples. [0.9962229496214758, 0.9962190943435388] . Seen 9148400 samples. [0.9963383757402504, 0.9962189305887329] . Seen 9256400 samples. [0.9965204596046517, 0.9962184507336732] . Seen 9519800 samples. [0.996792815736042, 0.9962184507336732] . Seen 9812400 samples. [0.9968482927047269, 0.9962184507336732] . Seen 9878200 samples. [0.9970070602553264, 0.9962184507336732] . Seen 10084000 samples. [0.9970850117886445, 0.9962184507336732] . Seen 10192600 samples. [0.99710092079875, 0.9962184507336732] . Seen 10215200 samples. [0.9971724799035275, 0.9962184507336732] . Seen 10320000 samples. [0.99719130697225, 0.9962185814127074] . Seen 10396800 samples. [0.9971923185765371, 0.9962176660085549] . Seen 10450000 samples. [0.9972998198530487, 0.9962176660085549] . Seen 10620600 samples. [0.997310844193802, 0.9962176660085549] . Seen 10639200 samples. [0.9973234328361537, 0.9962176660085549] . Seen 10660200 samples. [0.9974374569192718, 0.9962176660085549] . Seen 10860200 samples. [0.9974829003662036, 0.9962176660085549] . Seen 10944800 samples. [0.9975506087743713, 0.9962176660085549] . Seen 11211400 samples. [0.9975761706394671, 0.9962176660085549] . Seen 11280200 samples. [0.9975509221251254, 0.9962137371842981] . Seen 11523400 samples. [0.9975846680376191, 0.9962137186555163] . Seen 11676800 samples. [0.9975895758251948, 0.9962137186555163] . Seen 11687800 samples. [0.9976365680274141, 0.9962137186555163] . Seen 11798400 samples. [0.9976537270624685, 0.9962137186555163] . Seen 11838800 samples. [0.9977513576075107, 0.9962137186555163] . Seen 12080400 samples. [0.997755223798479, 0.9962137186555163] . Seen 12090400 samples. [0.997826162967847, 0.9962137186555163] . Seen 12280200 samples. [0.9978391153878619, 0.9962137186555163] . Seen 12316200 samples. [0.9978644927761431, 0.9962137186555163] . Seen 12388000 samples.\r"
     ]
    }
   ],
   "source": [
    "def test(model, loader, n_classes):\n",
    "    model=model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    seen_of_class = [0]*n_classes\n",
    "    acc = [0]*n_classes\n",
    "    #translator={0 : 'ddos', 1 : 'Benign'}\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, ground_truth in loader:\n",
    "            pred = torch.argmax(model(inputs.to(device)), dim=1)\n",
    "            targ = torch.argmax(ground_truth, dim=1)\n",
    "\n",
    "            targs = targ.tolist()\n",
    "            preds = pred.detach().tolist()\n",
    "\n",
    "            for p,t in zip(preds, targs):\n",
    "                seen_of_class[t] += 1\n",
    "                if p == t:\n",
    "                    acc[p] += 1\n",
    "\n",
    "            running_acc = [acc[i]/seen_of_class[i] if seen_of_class[i] != 0 else 0 for i in range(n_classes)]\n",
    "            print(\"Running accuracy:\", running_acc, \". Seen\", sum(seen_of_class), \"samples.\", end='\\r', flush=True)\n",
    "\n",
    "test(model, loader, 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a41f043e3f504d40bb31377d42299aba33beec9c14fb34c08aef092f76f6f6b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
