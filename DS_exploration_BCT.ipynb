{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preliminary and general analysis of train part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_selector='.\\\\datasets\\\\kaggle DDoS Dataset\\\\ddos_balanced'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serch_csvs_in_folder(path):\n",
    "    csv_files = []\n",
    "    for dirname, _, filenames in os.walk(path):\n",
    "        for filename in filenames:\n",
    "            csv_file = os.path.join(dirname, filename)\n",
    "            csv_files.append(csv_file)\n",
    "    return csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.\\\\datasets\\\\kaggle DDoS Dataset\\\\ddos_balanced\\\\final_dataset.csv']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_files=serch_csvs_in_folder(folder_selector)\n",
    "csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['6.328099204227328 GB']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes = [str(os.path.getsize(file)/(2.0**30)) + \" GB\" for file in csv_files]\n",
    "sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ddos': 6472647, 'Benign': 6321980}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_distribution_test = {}\n",
    "\n",
    "for ds in csv_files: #for every dataset\n",
    "    with pd.read_csv(ds, chunksize=10**6) as reader: #that has to be processed in chunks\n",
    "        for chunk in reader:\n",
    "            chunk.columns = chunk.columns.str.strip()\n",
    "\n",
    "            labels_count = chunk['Label'].value_counts()\n",
    "\n",
    "            for class_,count in labels_count.iteritems():\n",
    "                #print(class_, count)\n",
    "                if class_ in classes_distribution_test.keys():\n",
    "                    classes_distribution_test[class_] += count\n",
    "                else:\n",
    "                    classes_distribution_test[class_] = count\n",
    "\n",
    "classes_distribution_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_seen = {}\n",
    "dest='.\\\\datasets\\\\kaggle DDoS Dataset\\\\small\\\\final.csv'\n",
    "header_only=pd.read_csv(ds, nrows=0)\n",
    "header_only.to_csv(dest, mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_seen=classes_seen.fromkeys(classes_distribution_test, 0)\n",
    "for ds in csv_files: #for every dataset\n",
    "    with pd.read_csv(ds, chunksize=10**6) as reader: #that has to be processed in chunks\n",
    "        for chunk in reader:\n",
    "\n",
    "            rows_to_drop=[] #store rows of the chunk to cut away\n",
    "\n",
    "            ch_copy=chunk.copy() #duplicate chunk to edit it\n",
    "\n",
    "            #ch_copy.columns = ch_copy.columns.str.strip()\n",
    "\n",
    "            #drop na and +-inf values\n",
    "            ch_copy.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "            ch_copy.dropna(inplace=True)\n",
    "\n",
    "            #ch_copy.replace({'Label': replacement_dict}, inplace=True)\n",
    "            #drop useless cols\n",
    "            #ch_copy.drop(['Unnamed: 0','Timestamp','Source Port','Source IP','SimillarHTTP',\n",
    "             #'Protocol','Flow ID','Destination IP'], axis=1, inplace=True)\n",
    "\n",
    "            #print(chunk.index)\n",
    "            #print('Chunk length before drop: ', len(chunk))\n",
    "\n",
    "            for line_idx in ch_copy.index:\n",
    "                label = str(ch_copy.loc[line_idx, 'Label'])\n",
    "\n",
    "                classes_seen[label] += 1\n",
    "                if classes_seen[label] > 500000:\n",
    "                    rows_to_drop.append(line_idx)\n",
    "\n",
    "            ch_copy.drop(index=rows_to_drop, inplace=True) #finally drop excess\n",
    "\n",
    "            #print('Chunk length after drop: ', len(ch_copy))\n",
    "\n",
    "            #append with no header\n",
    "            ch_copy.to_csv(dest, mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.describe()\n",
    "df.info()\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row=df.iloc[0,:]\n",
    "first_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ACK Flag Count',\n",
       " 'Active Max',\n",
       " 'Active Mean',\n",
       " 'Active Min',\n",
       " 'Active Std',\n",
       " 'Average Packet Size',\n",
       " 'Avg Bwd Segment Size',\n",
       " 'Avg Fwd Segment Size',\n",
       " 'Bwd Avg Bulk Rate',\n",
       " 'Bwd Avg Bytes/Bulk',\n",
       " 'Bwd Avg Packets/Bulk',\n",
       " 'Bwd Header Length',\n",
       " 'Bwd IAT Max',\n",
       " 'Bwd IAT Mean',\n",
       " 'Bwd IAT Min',\n",
       " 'Bwd IAT Std',\n",
       " 'Bwd IAT Total',\n",
       " 'Bwd PSH Flags',\n",
       " 'Bwd Packet Length Max',\n",
       " 'Bwd Packet Length Mean',\n",
       " 'Bwd Packet Length Min',\n",
       " 'Bwd Packet Length Std',\n",
       " 'Bwd Packets/s',\n",
       " 'Bwd URG Flags',\n",
       " 'CWE Flag Count',\n",
       " 'Destination Port',\n",
       " 'Down/Up Ratio',\n",
       " 'ECE Flag Count',\n",
       " 'FIN Flag Count',\n",
       " 'Flow Bytes/s',\n",
       " 'Flow Duration',\n",
       " 'Flow IAT Max',\n",
       " 'Flow IAT Mean',\n",
       " 'Flow IAT Min',\n",
       " 'Flow IAT Std',\n",
       " 'Flow Packets/s',\n",
       " 'Fwd Avg Bulk Rate',\n",
       " 'Fwd Avg Bytes/Bulk',\n",
       " 'Fwd Avg Packets/Bulk',\n",
       " 'Fwd Header Length',\n",
       " 'Fwd Header Length.1',\n",
       " 'Fwd IAT Max',\n",
       " 'Fwd IAT Mean',\n",
       " 'Fwd IAT Min',\n",
       " 'Fwd IAT Std',\n",
       " 'Fwd IAT Total',\n",
       " 'Fwd PSH Flags',\n",
       " 'Fwd Packet Length Max',\n",
       " 'Fwd Packet Length Mean',\n",
       " 'Fwd Packet Length Min',\n",
       " 'Fwd Packet Length Std',\n",
       " 'Fwd Packets/s',\n",
       " 'Fwd URG Flags',\n",
       " 'Idle Max',\n",
       " 'Idle Mean',\n",
       " 'Idle Min',\n",
       " 'Idle Std',\n",
       " 'Init_Win_bytes_backward',\n",
       " 'Init_Win_bytes_forward',\n",
       " 'Label',\n",
       " 'Max Packet Length',\n",
       " 'Min Packet Length',\n",
       " 'PSH Flag Count',\n",
       " 'Packet Length Mean',\n",
       " 'Packet Length Std',\n",
       " 'Packet Length Variance',\n",
       " 'RST Flag Count',\n",
       " 'SYN Flag Count',\n",
       " 'Subflow Bwd Bytes',\n",
       " 'Subflow Bwd Packets',\n",
       " 'Subflow Fwd Bytes',\n",
       " 'Subflow Fwd Packets',\n",
       " 'Total Backward Packets',\n",
       " 'Total Fwd Packets',\n",
       " 'Total Length of Bwd Packets',\n",
       " 'Total Length of Fwd Packets',\n",
       " 'URG Flag Count',\n",
       " 'act_data_pkt_fwd',\n",
       " 'min_seg_size_forward']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = df.columns.str.strip().to_list()\n",
    "columns.sort()\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping duplicates, the length of df: 2522362\n"
     ]
    }
   ],
   "source": [
    "#df.drop(['Unnamed: 0','Timestamp','Source Port','Source IP','SimillarHTTP',\n",
    "#'Protocol','Flow ID','Destination IP'], axis=1, inplace=True)\n",
    "df.drop_duplicates(keep='first', inplace=True)\n",
    "print(\"After dropping duplicates, the length of df:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping NaNs, the length of df: 2520798\n"
     ]
    }
   ],
   "source": [
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "print(\"After dropping NaNs, the length of df:\", len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing column Bwd PSH Flags\n",
      "Removing column Bwd URG Flags\n",
      "Removing column Fwd Avg Bytes/Bulk\n",
      "Removing column Fwd Avg Packets/Bulk\n",
      "Removing column Fwd Avg Bulk Rate\n",
      "Removing column Bwd Avg Bytes/Bulk\n",
      "Removing column Bwd Avg Packets/Bulk\n",
      "Removing column Bwd Avg Bulk Rate\n"
     ]
    }
   ],
   "source": [
    "\"\"\"for col in df.columns:\n",
    "    if len(df[col].unique()) == 1:\n",
    "        print('Removing column', str(col))\n",
    "        df.drop(col,inplace=True,axis=1)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Analyzing train dataset part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_selector='.\\\\datasets\\\\CICDDoS2019\\\\original\\\\train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.\\\\datasets\\\\CICDDoS2019\\\\original\\\\train\\\\DrDoS_DNS.csv',\n",
       " '.\\\\datasets\\\\CICDDoS2019\\\\original\\\\train\\\\DrDoS_LDAP.csv',\n",
       " '.\\\\datasets\\\\CICDDoS2019\\\\original\\\\train\\\\DrDoS_MSSQL.csv',\n",
       " '.\\\\datasets\\\\CICDDoS2019\\\\original\\\\train\\\\DrDoS_NetBIOS.csv',\n",
       " '.\\\\datasets\\\\CICDDoS2019\\\\original\\\\train\\\\DrDoS_NTP.csv',\n",
       " '.\\\\datasets\\\\CICDDoS2019\\\\original\\\\train\\\\DrDoS_SNMP.csv',\n",
       " '.\\\\datasets\\\\CICDDoS2019\\\\original\\\\train\\\\DrDoS_SSDP.csv',\n",
       " '.\\\\datasets\\\\CICDDoS2019\\\\original\\\\train\\\\DrDoS_UDP.csv',\n",
       " '.\\\\datasets\\\\CICDDoS2019\\\\original\\\\train\\\\Syn.csv',\n",
       " '.\\\\datasets\\\\CICDDoS2019\\\\original\\\\train\\\\TFTP.csv',\n",
       " '.\\\\datasets\\\\CICDDoS2019\\\\original\\\\train\\\\UDPLag.csv']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_files = serch_csvs_in_folder(folder_selector)\n",
    "csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_17596\\2030347237.py:5: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_17596\\2030347237.py:5: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_17596\\2030347237.py:5: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_17596\\2030347237.py:5: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_17596\\2030347237.py:5: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_17596\\2030347237.py:5: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_17596\\2030347237.py:5: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_17596\\2030347237.py:5: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_17596\\2030347237.py:5: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_17596\\2030347237.py:5: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_17596\\2030347237.py:5: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [34], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m ds \u001b[39min\u001b[39;00m csv_files: \u001b[39m#for every dataset\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     \u001b[39mwith\u001b[39;00m pd\u001b[39m.\u001b[39mread_csv(ds, chunksize\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m6\u001b[39m) \u001b[39mas\u001b[39;00m reader: \u001b[39m#that has to be processed in chunks\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m         \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m reader:\n\u001b[0;32m      6\u001b[0m             chunk\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m chunk\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mstr\u001b[39m.\u001b[39mstrip()\n\u001b[0;32m      8\u001b[0m             labels_count \u001b[39m=\u001b[39m chunk[\u001b[39m'\u001b[39m\u001b[39mLabel\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalue_counts()\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\dl\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1187\u001b[0m, in \u001b[0;36mTextFileReader.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1185\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__next__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1186\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1187\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_chunk()\n\u001b[0;32m   1188\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m   1189\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\dl\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1284\u001b[0m, in \u001b[0;36mTextFileReader.get_chunk\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m   1282\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m\n\u001b[0;32m   1283\u001b[0m     size \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(size, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnrows \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_currow)\n\u001b[1;32m-> 1284\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mread(nrows\u001b[39m=\u001b[39;49msize)\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\dl\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1254\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1252\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[0;32m   1253\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1254\u001b[0m     index, columns, col_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(nrows)\n\u001b[0;32m   1255\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1256\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\dl\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:225\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    223\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    224\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[1;32m--> 225\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[0;32m    226\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    227\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\dl\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:817\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\dl\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:883\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\dl\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1026\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\dl\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1072\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\dl\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:1147\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\dl\\lib\\site-packages\\pandas\\core\\dtypes\\common.py:1429\u001b[0m, in \u001b[0;36mis_extension_array_dtype\u001b[1;34m(arr_or_dtype)\u001b[0m\n\u001b[0;32m   1420\u001b[0m     \u001b[39m# Note: if other EA dtypes are ever held in HybridBlock, exclude those\u001b[39;00m\n\u001b[0;32m   1421\u001b[0m     \u001b[39m#  here too.\u001b[39;00m\n\u001b[0;32m   1422\u001b[0m     \u001b[39m# NB: need to check DatetimeTZDtype and not is_datetime64tz_dtype\u001b[39;00m\n\u001b[0;32m   1423\u001b[0m     \u001b[39m#  to exclude ArrowTimestampUSDtype\u001b[39;00m\n\u001b[0;32m   1424\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39misinstance\u001b[39m(dtype, ExtensionDtype) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[0;32m   1425\u001b[0m         dtype, (DatetimeTZDtype, PeriodDtype)\n\u001b[0;32m   1426\u001b[0m     )\n\u001b[1;32m-> 1429\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_extension_array_dtype\u001b[39m(arr_or_dtype) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[0;32m   1430\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1431\u001b[0m \u001b[39m    Check if an object is a pandas extension array type.\u001b[39;00m\n\u001b[0;32m   1432\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1472\u001b[0m \u001b[39m    False\u001b[39;00m\n\u001b[0;32m   1473\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1474\u001b[0m     dtype \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(arr_or_dtype, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, arr_or_dtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "classes_distribution_train = {}\n",
    "\n",
    "for ds in csv_files: #for every dataset\n",
    "    with pd.read_csv(ds, chunksize=10**6) as reader: #that has to be processed in chunks\n",
    "        for chunk in reader:\n",
    "            chunk.columns = chunk.columns.str.strip()\n",
    "\n",
    "            labels_count = chunk['Label'].value_counts()\n",
    "\n",
    "            for class_,count in labels_count.iteritems():\n",
    "                #print(class_, count)\n",
    "                if class_ in classes_distribution_train.keys():\n",
    "                    classes_distribution_train[class_] += count\n",
    "                else:\n",
    "                    classes_distribution_train[class_] = count\n",
    "\n",
    "classes_distribution_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'classes_distribution_train' (dict)\n"
     ]
    }
   ],
   "source": [
    "dict_={}\n",
    "\n",
    "for k,v in classes_distribution_train.items():\n",
    "    dict_[k.replace('DrDoS_', '')] = classes_distribution_train[k]\n",
    "\n",
    "dict_['UDPLag'] = dict_.pop('UDP-lag')\n",
    "\n",
    "classes_distribution_train = dict_\n",
    "\n",
    "%store classes_distribution_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r classes_distribution_train\n",
    "plt.bar(classes_distribution_train.keys(), classes_distribution_train.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Analyzing test dataset part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_selector='.\\\\datasets\\\\CICDDoS2019\\\\original\\\\test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.\\\\datasets\\\\CICDDoS2019\\\\original\\\\test\\\\LDAP.csv',\n",
       " '.\\\\datasets\\\\CICDDoS2019\\\\original\\\\test\\\\MSSQL.csv',\n",
       " '.\\\\datasets\\\\CICDDoS2019\\\\original\\\\test\\\\NetBIOS.csv',\n",
       " '.\\\\datasets\\\\CICDDoS2019\\\\original\\\\test\\\\Portmap.csv',\n",
       " '.\\\\datasets\\\\CICDDoS2019\\\\original\\\\test\\\\Syn.csv',\n",
       " '.\\\\datasets\\\\CICDDoS2019\\\\original\\\\test\\\\UDP.csv',\n",
       " '.\\\\datasets\\\\CICDDoS2019\\\\original\\\\test\\\\UDPLag.csv']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_files = serch_csvs_in_folder(folder_selector)\n",
    "csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_5340\\770212701.py:5: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_5340\\770212701.py:5: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_5340\\770212701.py:5: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_5340\\770212701.py:5: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_5340\\770212701.py:5: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_5340\\770212701.py:5: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_5340\\770212701.py:5: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_5340\\770212701.py:5: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_5340\\770212701.py:5: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_5340\\770212701.py:5: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_5340\\770212701.py:5: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_5340\\770212701.py:5: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_5340\\770212701.py:5: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_5340\\770212701.py:5: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_5340\\770212701.py:5: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_5340\\770212701.py:5: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_5340\\770212701.py:5: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_5340\\770212701.py:5: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_5340\\770212701.py:5: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_5340\\770212701.py:5: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_5340\\770212701.py:5: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_5340\\770212701.py:5: DtypeWarning: Columns (85) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'LDAP': 1915122,\n",
       " 'NetBIOS': 3657497,\n",
       " 'BENIGN': 56965,\n",
       " 'MSSQL': 5787453,\n",
       " 'Portmap': 186960,\n",
       " 'Syn': 4891500,\n",
       " 'UDP': 3867155,\n",
       " 'UDPLag': 1873}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_distribution_test = {}\n",
    "\n",
    "for ds in csv_files: #for every dataset\n",
    "    with pd.read_csv(ds, chunksize=10**6) as reader: #that has to be processed in chunks\n",
    "        for chunk in reader:\n",
    "            chunk.columns = chunk.columns.str.strip()\n",
    "\n",
    "            labels_count = chunk['Label'].value_counts()\n",
    "\n",
    "            for class_,count in labels_count.iteritems():\n",
    "                #print(class_, count)\n",
    "                if class_ in classes_distribution_test.keys():\n",
    "                    classes_distribution_test[class_] += count\n",
    "                else:\n",
    "                    classes_distribution_test[class_] = count\n",
    "\n",
    "classes_distribution_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'classes_distribution_test' (dict)\n"
     ]
    }
   ],
   "source": [
    "%store classes_distribution_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r classes_distribution_test\n",
    "plt.bar(classes_distribution_test.keys(), classes_distribution_test.values())\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Manintain all data in original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "original='.\\\\datasets\\\\kaggle DDoS Dataset\\\\ddos_balanced\\\\final_dataset.csv'\n",
    "dest='.\\\\datasets\\\\kaggle DDoS Dataset\\\\refined_total\\\\refined.csv'\n",
    "header_only=pd.read_csv(original, nrows=0)\n",
    "header_only.drop(['Unnamed: 0','Timestamp','Src IP',\n",
    "            'Flow ID','Dst IP'], axis=1, inplace=True)\n",
    "header_only.to_csv(dest, mode='w', header=True, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Src Port', 'Dst Port', 'Protocol', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min', 'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min', 'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Tot', 'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max', 'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags', 'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Min', 'Pkt Len Max', 'Pkt Len Mean', 'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'SYN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'ECE Flag Cnt', 'Down/Up Ratio', 'Pkt Size Avg', 'Fwd Seg Size Avg', 'Bwd Seg Size Avg', 'Fwd Byts/b Avg', 'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg', 'Bwd Blk Rate Avg', 'Subflow Fwd Pkts', 'Subflow Fwd Byts', 'Subflow Bwd Pkts', 'Subflow Bwd Byts', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts', 'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean', 'Idle Std', 'Idle Max', 'Idle Min', 'Label']\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "print(header_only.columns.to_list())\n",
    "print(len(header_only.columns.to_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in csv_files: #for every dataset\n",
    "    with pd.read_csv(ds, chunksize=10**6) as reader: #that has to be processed in chunks\n",
    "        for chunk in reader:\n",
    "\n",
    "            ch_copy=chunk.copy() #duplicate chunk to edit it\n",
    "\n",
    "            ch_copy.columns = ch_copy.columns.str.strip()\n",
    "\n",
    "            #drop na and +-inf values\n",
    "            ch_copy.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "            ch_copy.dropna(inplace=True)\n",
    "\n",
    "            #drop useless cols\n",
    "            ch_copy.drop(['Unnamed: 0','Timestamp','Src IP',\n",
    "            'Flow ID','Dst IP'], axis=1, inplace=True)\n",
    "\n",
    "            #append with no header\n",
    "            ch_copy.to_csv(dest, mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5.202487978152931 GB']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sizes = [str(os.path.getsize(dest)/(2.0**30)) + \" GB\" for file in csv_files]\n",
    "sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_25564\\56861937.py:10: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for class_,count in labels_count.iteritems():\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_25564\\56861937.py:10: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for class_,count in labels_count.iteritems():\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_25564\\56861937.py:10: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for class_,count in labels_count.iteritems():\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_25564\\56861937.py:10: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for class_,count in labels_count.iteritems():\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_25564\\56861937.py:10: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for class_,count in labels_count.iteritems():\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_25564\\56861937.py:10: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for class_,count in labels_count.iteritems():\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_25564\\56861937.py:10: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for class_,count in labels_count.iteritems():\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_25564\\56861937.py:10: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for class_,count in labels_count.iteritems():\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_25564\\56861937.py:10: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for class_,count in labels_count.iteritems():\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_25564\\56861937.py:10: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for class_,count in labels_count.iteritems():\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_25564\\56861937.py:10: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for class_,count in labels_count.iteritems():\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_25564\\56861937.py:10: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for class_,count in labels_count.iteritems():\n",
      "C:\\Users\\MAX\\AppData\\Local\\Temp\\ipykernel_25564\\56861937.py:10: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for class_,count in labels_count.iteritems():\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ddos': 6472647, 'Benign': 6321980}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes_distribution_kaggle_bal = {}\n",
    "\n",
    "for ds in csv_files: #for every dataset\n",
    "    with pd.read_csv(ds, chunksize=10**6) as reader: #that has to be processed in chunks\n",
    "        for chunk in reader:\n",
    "            chunk.columns = chunk.columns.str.strip()\n",
    "\n",
    "            labels_count = chunk['Label'].value_counts()\n",
    "\n",
    "            for class_,count in labels_count.items():\n",
    "                #print(class_, count)\n",
    "                if class_ in classes_distribution_kaggle_bal.keys():\n",
    "                    classes_distribution_kaggle_bal[class_] += count\n",
    "                else:\n",
    "                    classes_distribution_kaggle_bal[class_] = count\n",
    "\n",
    "classes_distribution_kaggle_bal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'classes_distribution_kaggle_bal' (dict)\n"
     ]
    }
   ],
   "source": [
    "%store classes_distribution_kaggle_bal"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "a41f043e3f504d40bb31377d42299aba33beec9c14fb34c08aef092f76f6f6b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
