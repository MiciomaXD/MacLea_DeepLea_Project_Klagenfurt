{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import webdataset as wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='.\\\\datasets\\\\kaggle DDoS Dataset\\\\refined_total\\\\refined.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_total_len(data: str):\n",
    "    len_ = 0\n",
    "    with pd.read_csv(data, chunksize=10**6) as reader:\n",
    "        for chunk in reader:\n",
    "            len_ += len(chunk)\n",
    "    \n",
    "    return len_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12746847\n"
     ]
    }
   ],
   "source": [
    "from torch import tensor\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class NetMetrics(Dataset):\n",
    "    def __init__(self, path, X_cols, Y_cols, len_=None):\n",
    "        self.x_cols, self.y_cols = X_cols, Y_cols\n",
    "        self.path=path\n",
    "        self.len = get_total_len(path) if len_ == None else len_\n",
    "        print(self.len)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = pd.read_csv(self.path, skiprows=index-1, nrows=10)\n",
    "        #print(row.iloc[0, self.y_cols])\n",
    "        return tensor(row.iloc[0, self.x_cols], dtype=torch.float32), \\\n",
    "            row.iloc[0, self.y_cols][0]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "ds=NetMetrics(path, list(range(0,79)), [79], len_= 12_746_847)\n",
    "loader=DataLoader(ds, shuffle=True, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "ds=wd.WebDataset('datasets/kaggle DDoS Dataset/refined_total/refined.csv')\n",
    "loader=DataLoader(ds, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ReadError",
     "evalue": "('invalid header', <_io.BufferedReader name='datasets/kaggle DDoS Dataset/refined_total/refined.csv'>, 'datasets/kaggle DDoS Dataset/refined_total/refined.csv')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mf:\\miniconda\\envs\\dl\\lib\\tarfile.py:187\u001b[0m, in \u001b[0;36mnti\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    186\u001b[0m     s \u001b[39m=\u001b[39m nts(s, \u001b[39m\"\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 187\u001b[0m     n \u001b[39m=\u001b[39m \u001b[39mint\u001b[39;49m(s\u001b[39m.\u001b[39;49mstrip() \u001b[39mor\u001b[39;49;00m \u001b[39m\"\u001b[39;49m\u001b[39m0\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m8\u001b[39;49m)\n\u001b[0;32m    188\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: invalid literal for int() with base 8: 'Fwd Pkt'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidHeaderError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[1;32mf:\\miniconda\\envs\\dl\\lib\\tarfile.py:2336\u001b[0m, in \u001b[0;36mTarFile.next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2335\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 2336\u001b[0m     tarinfo \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarinfo\u001b[39m.\u001b[39;49mfromtarfile(\u001b[39mself\u001b[39;49m)\n\u001b[0;32m   2337\u001b[0m \u001b[39mexcept\u001b[39;00m EOFHeaderError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\dl\\lib\\tarfile.py:1122\u001b[0m, in \u001b[0;36mTarInfo.fromtarfile\u001b[1;34m(cls, tarfile)\u001b[0m\n\u001b[0;32m   1121\u001b[0m buf \u001b[39m=\u001b[39m tarfile\u001b[39m.\u001b[39mfileobj\u001b[39m.\u001b[39mread(BLOCKSIZE)\n\u001b[1;32m-> 1122\u001b[0m obj \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mfrombuf(buf, tarfile\u001b[39m.\u001b[39;49mencoding, tarfile\u001b[39m.\u001b[39;49merrors)\n\u001b[0;32m   1123\u001b[0m obj\u001b[39m.\u001b[39moffset \u001b[39m=\u001b[39m tarfile\u001b[39m.\u001b[39mfileobj\u001b[39m.\u001b[39mtell() \u001b[39m-\u001b[39m BLOCKSIZE\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\dl\\lib\\tarfile.py:1064\u001b[0m, in \u001b[0;36mTarInfo.frombuf\u001b[1;34m(cls, buf, encoding, errors)\u001b[0m\n\u001b[0;32m   1062\u001b[0m     \u001b[39mraise\u001b[39;00m EOFHeaderError(\u001b[39m\"\u001b[39m\u001b[39mend of file header\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1064\u001b[0m chksum \u001b[39m=\u001b[39m nti(buf[\u001b[39m148\u001b[39;49m:\u001b[39m156\u001b[39;49m])\n\u001b[0;32m   1065\u001b[0m \u001b[39mif\u001b[39;00m chksum \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m calc_chksums(buf):\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\dl\\lib\\tarfile.py:189\u001b[0m, in \u001b[0;36mnti\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m--> 189\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidHeaderError(\u001b[39m\"\u001b[39m\u001b[39minvalid header\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    190\u001b[0m \u001b[39mreturn\u001b[39;00m n\n",
      "\u001b[1;31mInvalidHeaderError\u001b[0m: invalid header",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mReadError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m loader:\n\u001b[0;32m      2\u001b[0m     \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39mshape())\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\dl\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\dl\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\dl\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:34\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m possibly_batched_index:\n\u001b[0;32m     33\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 34\u001b[0m         data\u001b[39m.\u001b[39mappend(\u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset_iter))\n\u001b[0;32m     35\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mended \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\dl\\lib\\site-packages\\webdataset\\pipeline.py:68\u001b[0m, in \u001b[0;36mDataPipeline.iterator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39m\"\"\"Create an iterator through the entire dataset, using the given number of repetitions.\"\"\"\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepetitions):\n\u001b[1;32m---> 68\u001b[0m     \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterator1():\n\u001b[0;32m     69\u001b[0m         \u001b[39myield\u001b[39;00m sample\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\dl\\lib\\site-packages\\webdataset\\tariterators.py:150\u001b[0m, in \u001b[0;36mgroup_by_keys\u001b[1;34m(data, keys, lcase, suffixes, handler)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[39m\"\"\"Return function over iterator that groups key, value pairs into samples.\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \n\u001b[0;32m    146\u001b[0m \u001b[39m:param keys: function that splits the key into key and extension (base_plus_ext)\u001b[39;00m\n\u001b[0;32m    147\u001b[0m \u001b[39m:param lcase: convert suffixes to lower case (Default value = True)\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    149\u001b[0m current_sample \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 150\u001b[0m \u001b[39mfor\u001b[39;00m filesample \u001b[39min\u001b[39;00m data:\n\u001b[0;32m    151\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(filesample, \u001b[39mdict\u001b[39m)\n\u001b[0;32m    152\u001b[0m     fname, value \u001b[39m=\u001b[39m filesample[\u001b[39m\"\u001b[39m\u001b[39mfname\u001b[39m\u001b[39m\"\u001b[39m], filesample[\u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\dl\\lib\\site-packages\\webdataset\\tariterators.py:137\u001b[0m, in \u001b[0;36mtar_file_expander\u001b[1;34m(data, handler)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m exn:\n\u001b[0;32m    136\u001b[0m     exn\u001b[39m.\u001b[39margs \u001b[39m=\u001b[39m exn\u001b[39m.\u001b[39margs \u001b[39m+\u001b[39m (source\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m), source\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39murl\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m--> 137\u001b[0m     \u001b[39mif\u001b[39;00m handler(exn):\n\u001b[0;32m    138\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m    139\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\dl\\lib\\site-packages\\webdataset\\filters.py:80\u001b[0m, in \u001b[0;36mreraise_exception\u001b[1;34m(exn)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mreraise_exception\u001b[39m(exn):\n\u001b[0;32m     76\u001b[0m     \u001b[39m\"\"\"Reraises the given exception; used as a handler.\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \n\u001b[0;32m     78\u001b[0m \u001b[39m    :param exn: exception\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m     \u001b[39mraise\u001b[39;00m exn\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\dl\\lib\\site-packages\\webdataset\\tariterators.py:129\u001b[0m, in \u001b[0;36mtar_file_expander\u001b[1;34m(data, handler)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(source, \u001b[39mdict\u001b[39m)\n\u001b[0;32m    128\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m source\n\u001b[1;32m--> 129\u001b[0m \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m tar_file_iterator(source[\u001b[39m\"\u001b[39m\u001b[39mstream\u001b[39m\u001b[39m\"\u001b[39m], handler\u001b[39m=\u001b[39mhandler):\n\u001b[0;32m    130\u001b[0m     \u001b[39massert\u001b[39;00m (\n\u001b[0;32m    131\u001b[0m         \u001b[39misinstance\u001b[39m(sample, \u001b[39mdict\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m sample \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mfname\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m sample\n\u001b[0;32m    132\u001b[0m     )\n\u001b[0;32m    133\u001b[0m     sample[\u001b[39m\"\u001b[39m\u001b[39m__url__\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m url\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\dl\\lib\\site-packages\\webdataset\\tariterators.py:88\u001b[0m, in \u001b[0;36mtar_file_iterator\u001b[1;34m(fileobj, skip_meta, handler)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtar_file_iterator\u001b[39m(fileobj, skip_meta\u001b[39m=\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m__[^/]*__($|/)\u001b[39m\u001b[39m\"\u001b[39m, handler\u001b[39m=\u001b[39mreraise_exception):\n\u001b[0;32m     82\u001b[0m     \u001b[39m\"\"\"Iterate over tar file, yielding filename, content pairs for the given tar stream.\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \n\u001b[0;32m     84\u001b[0m \u001b[39m    :param fileobj: byte stream suitable for tarfile\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[39m    :param skip_meta: regexp for keys that are skipped entirely (Default value = r\"__[^/]*__($|/)\")\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \n\u001b[0;32m     87\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m     stream \u001b[39m=\u001b[39m tarfile\u001b[39m.\u001b[39;49mopen(fileobj\u001b[39m=\u001b[39;49mfileobj, mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mr|*\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     89\u001b[0m     \u001b[39mfor\u001b[39;00m tarinfo \u001b[39min\u001b[39;00m stream:\n\u001b[0;32m     90\u001b[0m         fname \u001b[39m=\u001b[39m tarinfo\u001b[39m.\u001b[39mname\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\dl\\lib\\tarfile.py:1650\u001b[0m, in \u001b[0;36mTarFile.open\u001b[1;34m(cls, name, mode, fileobj, bufsize, **kwargs)\u001b[0m\n\u001b[0;32m   1648\u001b[0m stream \u001b[39m=\u001b[39m _Stream(name, filemode, comptype, fileobj, bufsize)\n\u001b[0;32m   1649\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1650\u001b[0m     t \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(name, filemode, stream, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1651\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m   1652\u001b[0m     stream\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\dl\\lib\\tarfile.py:1531\u001b[0m, in \u001b[0;36mTarFile.__init__\u001b[1;34m(self, name, mode, fileobj, format, tarinfo, dereference, ignore_zeros, encoding, errors, pax_headers, debug, errorlevel, copybufsize)\u001b[0m\n\u001b[0;32m   1529\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1530\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfirstmember \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1531\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfirstmember \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnext()\n\u001b[0;32m   1533\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   1534\u001b[0m     \u001b[39m# Move to the end of the archive,\u001b[39;00m\n\u001b[0;32m   1535\u001b[0m     \u001b[39m# before the first empty block.\u001b[39;00m\n\u001b[0;32m   1536\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n",
      "File \u001b[1;32mf:\\miniconda\\envs\\dl\\lib\\tarfile.py:2348\u001b[0m, in \u001b[0;36mTarFile.next\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2346\u001b[0m         \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m   2347\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moffset \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 2348\u001b[0m         \u001b[39mraise\u001b[39;00m ReadError(\u001b[39mstr\u001b[39m(e))\n\u001b[0;32m   2349\u001b[0m \u001b[39mexcept\u001b[39;00m EmptyHeaderError:\n\u001b[0;32m   2350\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moffset \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "\u001b[1;31mReadError\u001b[0m: ('invalid header', <_io.BufferedReader name='datasets/kaggle DDoS Dataset/refined_total/refined.csv'>, 'datasets/kaggle DDoS Dataset/refined_total/refined.csv')"
     ]
    }
   ],
   "source": [
    "for x in loader:\n",
    "    print(x.shape())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a41f043e3f504d40bb31377d42299aba33beec9c14fb34c08aef092f76f6f6b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
